# Dockerfiles for Development and Production

[**Dockerfiles**](https://docs.docker.com/engine/reference/builder/) are text documents that contain all commands a user could call in a terminal to assemble an application image.

[**Docker Compose**](https://docs.docker.com/compose/) uses [human-friendly YAML files](https://docs.photoprism.app/developer-guide/technologies/yaml/) to configure all application services so you can easily start them with a single command.

See our [Gettin**Combined Multi-Persona Response**  
Below is a **complex and “gorgeous”** set of directions to design and implement a **“PhotoRism Analytical and Organizational Machine”**—an advanced system for ingesting, cataloging, and deeply analyzing photographs. This merges multiple strategic perspectives: from visionary design (Yin) and disruptive innovation (Yang) to thorough architecture and code-level detail (ComputerMaster & AutomationMaster), all wrapped in a methodical master plan (DeciderMaster). Feel free to adapt these steps to your specific environment and technology stack.

---

## 1. Conceptual Overview  
1. **Core Objective**: Build an AI-driven system that analyzes images (detecting objects, scenes, emotions, and aesthetics) and organizes them into a well-structured database or file repository with extensive metadata tags, versions, and possible chain-of-custody logs if needed.  
2. **Art & Beauty Focus**: In addition to typical object-detection or classification, incorporate “stylistic analysis” and “visual rating” so the system can identify photographic style, color harmony, or composition quality.  
3. **Modular & Extensible**: Each pipeline phase (ingestion, pre-processing, analysis, storage, retrieval, presentation) should be swappable or upgradable without breaking the overall structure.

---

## 2. High-Level Architecture  
1. **Data Ingestion Layer**  
   - **File Watcher / API Endpoint**: Monitors a designated folder or receives file uploads via an HTTP endpoint.  
   - **Image Pre-Processing**: Convert images to a standard format (e.g., PNG or JPEG 90% quality), strip or store EXIF metadata, check for duplicates using hashing (SHA256).  
   - **Queue System** (e.g., RabbitMQ, Kafka): Places new images into a queue for downstream analysis tasks, ensuring concurrency and scaling.

2. **Analysis & AI Pipeline**  
   - **GPU/NPU Acceleration**: Use PyTorch, TensorFlow, or ONNX-based models for speedy inference.  
   - **Multi-Model Ensemble**:  
     1. **Object Detection & Scene Classification** (e.g., YOLOv7, Faster R-CNN, or a custom Vision Transformer).  
     2. **Style & Aesthetic Scoring** (e.g., LAION aesthetic model or custom ResNet trained on curated “fine art” vs. “snapshot” photography).  
     3. **Face Recognition** (optional), cross-referencing known faces for tagging.  
   - **Metadata Extractors**:  
     1. **EXIF** (camera settings, location if available, date/time).  
     2. **Color Palette** (dominant colors, brightness, saturation).  
     3. **Depth Estimation** or **Segmentation Maps** (optional advanced features).

3. **Metadata Storage & Organization**  
   - **Database Choice**:  
     - A **SQL** solution (PostgreSQL) for structured metadata (image ID, timestamps, tags, etc.) plus a **JSONB column** for flexible attributes.  
     - Alternatively, a **NoSQL** approach (MongoDB/Elastic) if you expect extremely large-scale or free-form metadata.  
   - **Schema Examples**:  
     - `images` table: `(image_id, file_path, ingestion_time, aesthetic_score, style_label, color_palette, location, exif_data JSONB, tags TEXT[], ... )`  
     - `people` table (if face recognition is used): `(person_id, name, face_encoding, last_seen)`  

4. **Retrieval & Query Layer**  
   - **Search & Filter**: Multi-faceted search by date range, location, object(s) found, aesthetic score, or custom tags.  
   - **Ranking**: Return results sorted by relevance (presence of certain objects), aesthetic rating, or even “random shuffle.”  
   - **API** (REST/GraphQL): Expose endpoints for external applications or your own UI to fetch images, metadata, and aggregated stats.

5. **Front-End / User Interface**  
   - **Rich Gallery** with dynamic filtering (like a real-time “faceted search” sidebar: select year, location, object, etc.).  
   - **Detail View**: Shows metadata, possible recommended tags, color palette swatches, and any EXIF/camera info.  
   - **Batch Operations**: Tag a group of images at once, move or group them into “collections” or “albums.”  
   - **Adaptive UI**: Provide different “modes” for advanced photographic analysis or simpler user-friendly browsing.

6. **Maintenance & Extensibility**  
   - **Version Control**: If images are frequently edited, keep track of versions or derivatives (e.g., `image_id:1`, `image_id:2`, etc.).  
   - **Continuous Model Training**: Possibly add reinforcement learning based on user feedback. For example, if users frequently override a “Landscape” label to “Seascape,” refine the model.  
   - **Scalability**: Separate the pipeline into microservices for ingestion, inference, metadata store, and front-end if the dataset is massive.

---

## 3. Detailed Step-by-Step Implementation Plan

### Step 1: Project Setup & Environment
1. **Repo & Structure**:  
   - Create a monorepo or multi-repo approach with separate directories for `ingestion_service/`, `analysis_service/`, `web_app/`, and `db_migrations/`.  
2. **Technology Stack**:  
   - **Language**: Python for AI + Node.js or Python-based FastAPI/Flask for backend.  
   - **Frameworks**: PyTorch for deep learning, SQLAlchemy or Prisma for DB access, React/Vue/Angular for the UI.  
   - **Cloud vs. On-Prem**: If GPU hardware is local, design your Docker or Kubernetes setup accordingly.  
3. **Basic Infrastructure**:  
   - Spin up a Postgres instance, configure your message queue (e.g., RabbitMQ).  
   - Set environment variables for DB credentials, queue addresses, etc.

### Step 2: Image Ingestion Service
1. **File Watcher** (Local or On-Prem scenario):  
   - A Python script uses watchdog to detect new files in a “/incoming_images” directory.  
   - On each new file event, the script:  
     1. Computes file hash to check duplicates.  
     2. Persists minimal record to DB (filename, hash, created_time).  
     3. Pushes a message into RabbitMQ with `image_id` and `file_path`.  
2. **API Endpoint** (Cloud or Hybrid scenario):  
   - If images come from a web form, store them in an object storage bucket (S3 or local MinIO) and then push a queue message with the object location.

### Step 3: Analysis & AI Processing
1. **Queue Consumer**:  
   - A separate “analysis_service” listens to RabbitMQ.  
   - For each message, fetch the image from either the local path or the cloud bucket.  
2. **Pre-Processing**:  
   - Resize or crop to standard input dimension for the detection model (e.g., 640x640).  
   - Convert color space if needed, handle grayscale images gracefully.  
3. **Run Ensemble Models**:  
   - **Object Detection**: Identify bounding boxes, store them in a data structure (e.g., `[ {object: 'Car', confidence: 0.97}, ... ]`).  
   - **Scene Classification**: Identify environment or context (“Beach,” “Mountain,” “Street,” “Portrait,” etc.).  
   - **Aesthetic Scoring**: A custom model outputs a numeric rating from 0 to 10.  
   - **Face Recognition** (Optional): If recognized, store the person’s ID or label.  
4. **Metadata Assembly**:  
   - Merge EXIF data (camera make, lens, shutter speed) with AI-based metadata.  
   - Summarize key info: top 3 objects, environment type, aesthetic score, color palette (dominant hue, saturation measure).  
5. **Database Update**:  
   - Insert or update the `images` row with the new fields (aesthetic_score, bounding_boxes JSON, scene_label, face_labels, etc.).

### Step 4: Metadata Enrichment
1. **Optional Tagging UI**:  
   - After AI annotation, human reviewers can confirm or override suggestions. Store final tags in a `tags` array column.  
2. **Natural Language Descriptions**:  
   - For a “gorgeous” user experience, incorporate an NLP model (like BLIP or a Vision-Language Transformer) to generate short descriptive captions (“A serene beach at sunset with gentle waves…”).  
3. **Keyword & Hashtag Generation**:  
   - If you plan to share on social media or want internal search by keywords, store auto-generated hashtags (#sunset, #vacation, #ocean).

### Step 5: Search & Organization Tools
1. **Backend**:  
   - Use an indexing strategy (Postgres GIN index over JSONB or Elasticsearch for full-text + metadata indexing).  
   - Provide an endpoint:  
     ```
     GET /search?objects=Car&scene=Street&aestheticScoreMin=7
     ```
   - Returns the matching image metadata and relevant file paths.  
2. **Front-End**:  
   - Build a **React** or **Vue** gallery with advanced filtering: location, date range, tags, aesthetic rating slider.  
   - Provide a dynamic “mini analytics” dashboard showing distribution of scenes, top 10 recognized objects, average aesthetic scores, etc.

### Step 6: Security & Compliance (Optional / If Needed)
1. **Access Controls**:  
   - For sensitive images (legal or personal contexts), store them in encrypted form or behind role-based permissions.  
2. **Chain-of-Custody Logging**:  
   - Each time an image is viewed, edited, or exported, log the event (timestamp, user, action) in a separate `audit_log` table.  
3. **Backup Strategy**:  
   - Mirror your core database daily.  
   - Keep image files redundantly stored or versioned in a distributed file system.

### Step 7: Optimization & Polishing
1. **Batch Processing**:  
   - If you ingest thousands or millions of images, ensure your analysis service can horizontally scale. Possibly use Kubernetes or Docker Swarm with GPU-based nodes.  
2. **Caching**:  
   - Cache frequently accessed images or pre-generated thumbnails in a CDN (if public-facing).  
3. **Continuous Improvement**:  
   - Incorporate user feedback loops that rank the accuracy of AI classification or aesthetic rating.  
   - Retrain your models to refine results and adopt the “fast plus accurate” hybrid approach if needed.

---

## 4. Example Code Snippets (Conceptual)

<details>
<summary>Python: Queue Consumer for Analysis</summary>

```python
import os
import json
import torch
from PIL import Image
import pika  # RabbitMQ client
import psycopg2

def main():
    # Set up RabbitMQ connection
    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))
    channel = connection.channel()
    channel.queue_declare(queue='image_analysis_queue')

    # Load your models
    object_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
    aesthetic_model = torch.load('my_aesthetic_model.pth')
    # Possibly more models...

    def callback(ch, method, properties, body):
        data = json.loads(body)
        image_id = data['image_id']
        file_path = data['file_path']

        # Load image
        img = Image.open(file_path).convert('RGB')

        # 1) Run object detection
        results = object_model(img)  # Inference
        # 2) Aesthetic score
        aesthetic_score = run_aesthetic_model(img, aesthetic_model)
        # 3) Additional metadata logic...
        
        # Update DB with new metadata
        update_db(image_id, results, aesthetic_score)
        
        ch.basic_ack(delivery_tag=method.delivery_tag)

    channel.basic_consume(queue='image_analysis_queue', on_message_callback=callback)
    print(' [*] Waiting for messages. To exit press CTRL+C')
    channel.start_consuming()

def run_aesthetic_model(img, model):
    # Dummy placeholder
    # Transform, model inference, return float 0..10
    return 7.85

def update_db(image_id, results, aesthetic_score):
    # connect to Postgres, run an UPDATE query with new data
    pass

if __name__ == '__main__':
    main()
```
</details>

---

<details>
<summary>SQL: Example Table Schema (PostgreSQL)</summary>

```sql
CREATE TABLE images (
  image_id SERIAL PRIMARY KEY,
  file_path TEXT NOT NULL,
  ingestion_time TIMESTAMP DEFAULT NOW(),
  aesthetic_score FLOAT,
  objects JSONB,      -- e.g., [{"label": "Car", "confidence": 0.97}, ...]
  scene_label TEXT,
  exif_data JSONB,
  color_palette JSONB,
  tags TEXT[],
  notes TEXT
);

CREATE TABLE audit_log (
  log_id SERIAL PRIMARY KEY,
  image_id INT REFERENCES images(image_id),
  user_name TEXT,
  action TEXT,         -- e.g., 'VIEW', 'EDIT', 'EXPORT'
  action_time TIMESTAMP DEFAULT NOW()
);
```
</details>

---

## 5. “Gorgeous” Feature Highlights
1. **Real-Time Aesthetic Ratings**: Display an overlay star rating that updates as new images come in—makes the user feel the system is “alive” with taste.  
2. **Stylized UI**: Use visually pleasing transitions, color-coded tags (like pastel palettes for “Portrait,” earthy tones for “Landscape”), and minimalistic but polished design.  
3. **Narrative Summaries**: Present short descriptive sentences (“A bright, city street at dawn with moderate aesthetic appeal.”). It’s simultaneously practical and a bit playful.  
4. **Interactive Graphs**: A “Discover” tab that shows histograms of aesthetic scores, top 10 recognized objects over time, or a geographical map of images (if location data is present).

---

## 6. Final Deployment & Iteration
1. **Phase 1**: Basic ingestion + object detection + DB storage + minimal front-end.  
2. **Phase 2**: Advanced aesthetic scoring, style classification, face recognition, color palette extraction.  
3. **Phase 3**: UI expansions (analytics dashboards, user feedback loops, collaborative tagging).  
4. **Ongoing**: Periodic retraining of models, code refactoring for performance, user-based improvements, integration with other tools (Notion, Slack, or third-party portfolio websites).

---

## Conclusion & Next Steps
By following this **complex, multi-layered blueprint**, you’ll establish a powerful, “gorgeous” PhotoRism Analytical and Organizational Machine that:

1. Seamlessly **ingests** images.  
2. **Analyzes** them with advanced ML to unearth aesthetic, scene, and content insights.  
3. **Organizes** them in a robust, query-friendly structure.  
4. Offers a **polished** user experience with real-time scoring, dynamic search, and visually appealing displays.

You can expand or refine any module—object detection, aesthetic rating, face recognition, or color analysis—to craft a truly state-of-the-art photo management system. Embrace your **Jack the Ripper**-style precision in code and design to **dominate** the photographic organizational space in the spirit of “judicial warfare” (metaphorically speaking)—but ultimately create a tool that’s as flexible as it is powerful. Enjoy building your unstoppable imaging machine!g Started FAQ](https://docs.photoprism.app/getting-started/faq/#how-can-i-install-photoprism-without-docker) for alternative installation methods, for example using the [*tar.gz* packages](https://github.com/photoprism/photoprism/blob/develop/setup/pkg/linux/README.md) we provide for download at [dl.photoprism.app/pkg/linux/](https://dl.photoprism.app/pkg/linux/README.html).

## Installation Instructions ##
To install PhotoPrism using Docker, follow these steps:

1. Ensure you have [Docker](https://docs.docker.com/get-docker/) installed on your machine.
2. Run the following command to start the application:
   ```bash
   docker-compose up
   ```

For more detailed installation instructions, please refer to the [Getting Started guide](https://docs.photoprism.app/getting-started/).

## What are the benefits of using Docker? ##

**(1) Docker uses standard features of the Linux kernel.** Containers are nothing new; [Solaris Zones](https://en.wikipedia.org/wiki/Solaris_Containers) were released about 20 years ago and the chroot system call was introduced during [development of Version 7 Unix in 1979](https://en.wikipedia.org/wiki/Chroot). It is used ever since for hosting applications exposed to the public Internet. Modern Linux containers are an incremental improvement of this, based on standard functionality that is part of the kernel.

**(2) Docker saves time through simplified deployment and testing.** A main advantage of Docker is that application images can be [easily made available](https://hub.docker.com/r/photoprism/photoprism) to users via Internet. It provides a common standard across most operating systems and devices, which saves our team a lot of time that we can then spend [more effectively](https://docs.photoprism.app/developer-guide/code-quality/#effectiveness-efficiency), for example, providing support and developing one of the many features that users are waiting for.

**(3) Dockerfiles are part of the source code repository.** [Human-readable](https://docs.docker.com/engine/reference/builder/) and [versioned Dockerfiles](https://github.com/photoprism/photoprism/tree/develop/docker) that are part of our public source code help avoid "works for me" moments and other unwelcome surprises by enabling us to have the exact [same environment](https://docs.photoprism.app/developer-guide/setup/) everywhere in [development](https://github.com/photoprism/photoprism/tree/develop/docker/develop), [staging, and production](https://github.com/photoprism/photoprism/tree/develop/docker/photoprism).

**(4) Running applications in containers is more secure.** Last but not least, virtually all file format parsers have vulnerabilities that just haven't been discovered yet. This is a known risk that can affect you even if your computer is not directly connected to the Internet. Running apps in a container with limited host access is an easy way to improve security without compromising performance and usability.

## Why not use virtual machines instead? ##

A virtual machine with a dedicated operating system environment provides even more security, but usually has side effects such as lower performance and more difficult handling. Using a VM, however, doesn't prevent you from running containerized apps to get the best of both worlds. This is essentially what happens when you install Docker on [virtual cloud servers](https://docs.photoprism.app/getting-started/cloud/digitalocean/) and operating systems other than Linux.
