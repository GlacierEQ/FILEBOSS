# DeepSeek-Coder Environment Configuration

# Model Configuration
MODEL_SIZE=base  # base, large
MODEL_PATH=/app/models
TORCH_COMPILE=1  # Enable torch.compile for faster inference
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128  # Memory optimization for large models

# Quantization Settings
QUANTIZATION_TYPE=none  # none, 4bit, 8bit
QUANTIZE_WITH=bitsandbytes  # bitsandbytes, gguf

# Server Configuration
PORT=8000
HOST=0.0.0.0
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
WORKERS=1  # Number of API worker processes

# Directory Configuration
DATA_DIR=/app/data
LOG_DIR=/app/logs
CACHE_DIR=/app/cache

# Security Configuration
ENABLE_AUTH=true  # Set to true in production
API_KEYS=sk-your-api-key-1,sk-your-api-key-2  # Comma-separated list of API keys
RATE_LIMIT=100  # Requests per hour per API key

# CORS Configuration
ALLOW_ORIGINS=*  # Use comma-separated list of domains in production
ALLOW_CREDENTIALS=false
ALLOW_METHODS=GET,POST,OPTIONS
ALLOW_HEADERS=Content-Type,Authorization

# Performance Configuration
MAX_BATCH_SIZE=4  # Maximum batch size for inference
MAX_CONCURRENT_REQUESTS=4  # Maximum number of concurrent requests
REQUEST_TIMEOUT=300  # Request timeout in seconds

# Elasticsearch Configuration
ELASTICSEARCH_HOST=elasticsearch
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_USERNAME=
ELASTICSEARCH_PASSWORD=
ELASTICSEARCH_USE_SSL=false
ELASTICSEARCH_INDEX_PREFIX=deepseek-

# Monitoring Configuration
ENABLE_METRICS=true  # Enable Prometheus metrics
ENABLE_TELEMETRY=true  # Enable anonymous usage statistics
PROMETHEUS_PORT=9090

# GPU Configuration
CUDA_VISIBLE_DEVICES=all  # GPU device IDs to use, comma-separated, or 'all'
